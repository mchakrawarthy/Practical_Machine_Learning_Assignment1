---
output:
  html_document:
    keep_md: yes
---

# Predicting the Class of Weight Lifting Exercises

### Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 

### Goal

This study involves exploring weight lifting exercise data. There are 160 variables (columns in the data). Each observation is classified with "classe" variable, which has 5 levels: A, B, C, D, E. The goal of this study is to come up with a prediction algorithm to accurately predict the class for each observation.

### Approach

1. Divide the training set into 2 sets - 80:20 (train, test).  
2. Use training set for training random forest model.
3. Predict and create ConfusionMatrix on **test set** to measure performance of model.
4. Predict using the best performing model in Step 3, classification for 20 test samples. Create the submission files

### Load data and pre process

We load the data and inspect it. Inspection of the data showed that there are many columns mostly with NA and empty values. Clearly, we need to remove those variables from our study, as they will not provide any information for the prediction algorithm. This step also removes some observations where "DIVby0" error is detected. Luckily, those errors occur in the variables with mostly NA and blank values. Since we do this cleaning in training data set, we should do the same for testing data set.

```{r load_libraries,results='hide',warning=FALSE}
library(caret) 
library(foreach)
library(doSNOW)
ncores <- 2
set.seed(1234)
```

```{r loading_data, cache=TRUE}
setwd("C:\\Users\\Chakri\\Desktop\\Coursera\\Practical Machine Learning")
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
              destfile="pml-training.csv",
              mode="wb")
# load data
rawData <- read.csv("pml-training.csv", na.strings=c("", "NA", "#DIV/0!"))
# discard NAs
na.count <- apply(rawData, 2, function(x) {sum(is.na(x))}) 
validData <- rawData[, which(na.count == 0)]

# remove useless predictors
removeColumns <- grep("timestamp|X|user_name|new_window|num_window", names(validData))
validData <- validData[ ,-removeColumns]

# make training set 80
trainIndex <- createDataPartition(y = validData$classe, p=0.80, list=FALSE)
trainData  <- validData[trainIndex, ]
testData   <- validData[-trainIndex, ]

classeColumnIndex <- grep("classe", names(testData))
dim(trainData); dim(testData)

```

### Prediction Modeling

We decided to use random forest algorithm. We utilize 10-fold cross-validation to train algorithm.

```{r randomForest_model, cache=TRUE}
# power up cluster
cl <- makeCluster(ncores)
registerDoSNOW(cl)

cv_opts <- trainControl(method="cv", number=10, allowParallel=TRUE)

# run randomforest model
system.time(
        model_rf <- train(classe ~ ., data = trainData, 
                method="rf", trControl = cv_opts)
        )


# stop cluster
stopCluster(cl)

model_rf

```

Apply the model on the train data set and see how it performs.

```{r randomForest_prediction_train}

preds_rf_train <- predict(model_rf, trainData[, -classeColumnIndex])
confusionMatrix(preds_rf_train, trainData[, classeColumnIndex])

```

Woh, thats pretty good accuracy. So we are glad we pick random forest.

```{r rf_model_analysis}
imp <- varImp(model_rf$finalModel)
varImpPlot(model_rf$finalModel,
main="Variable Importance",
sort=TRUE,
n.var=25)
abline(v=mean(imp$Overall),col="red")

```

In an extended version, we can include a prediction algorithm with only these top variables (instead of all 52 variables) and determine its accuracy. But we have already exceeded the limitation of this report. The following plot shows how the accuracy of the algorithm affected by the number of trees used.

```{r, plot_tree}
plot(model_rf$finalModel)
```

### Model Testing

```{r randomForest_prediction}
preds_rf <- predict(model_rf, testData[, -classeColumnIndex])
confusionMatrix(preds_rf, testData[, classeColumnIndex])
```

As you can see, the accuracy is 99.4%[]=-. This means the out-sample error is 0.6%. This is a pretty good result.

### Results

We have the best prediction from Random Forest, so we select Random Forest as our final model.

### Predicting outcome on given 20 test cases
```{r final_prediction}
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
              destfile="pml-testing.csv",
              mode="wb")

# Predict the outcomes in the given test file
classetestData <- read.csv("pml-testing.csv", na.strings=c("", "NA", "#DIV/0!"))
removeColumns <- grep("timestamp|X|user_name|new_window|num_window", names(classetestData))
classetestData <- classetestData[ ,-removeColumns]
answers <- predict(model_rf, classetestData)
answers

# write output to files

pml_write_files <- function(answers){
        x <- as.character(answers)
        n = length(x)
        for(i in 1:n){
                filename = paste0("problem_id_",i,".txt")
                write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
        }
}

pml_write_files(answers)

```
